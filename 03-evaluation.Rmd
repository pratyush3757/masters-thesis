---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/template.tex
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Evaluation
\minitoc <!-- this will include a mini table of contents-->

## Qualitative Security Analysis

For our case, an attacker either controls the Prv or is a malicious Prv that changes
the control flow of the program, while trying to keep the CFlog intact 
or succeeds in completing malicious operations before Vrf gets the report.
A weaker set of these guarantees provided by BLAST, target an attacker that changes the control flow, 
but cares about not being detected.
As described in sections [2.2 - Adding Integrity](#adding-integrity) and 
[2.3 - Optimizing Blast](#optimizing-blast), we add on top this weaker set of guarantees,
to provide the integrity guarantees.

Code corruption attacks that would replace the code can be prevented
as the function calls and returns would generate exceptions without properly signed
addresses. And even still, attacks would be detected as the call/return addresses and
the path numbers collected in the CFlog would not match. For code injection attacks,
we assume that Prv has data-execution prevention implemented in the REE.
For ensuring the integrity of the CFlog, all memory stores from code already inside the
program are followed by SFI instrumentation.

As for the possibility of code-reuse attacks, the target surface for any jumps is
decreased just to `bti` instructions, while the jumps (and returns) themselves are
protected by pointer authentication. The increase in binary sizes from enabling BTI is ~2% [@arm-pacbti],
that means the jump target surface for code-reuse attacks decreases to that amount.
Attacker would need to target those gadgets, while also considering 
pointer authentication. Any successfull attacks bypassing these measures, would
be detected in the CFlog due to BLAST instrumentation. To prevent detection, 
attacker would also need to find gadgets in the reduced subset to modify `BLReg` and `LogReg`.
BLAST already provides guarantees against bypassing detection [@blast].

Attacks to program data to alter the control flow of a program is detected in the CFlog.
But, it would still be possible to alter the sensitive variables in a way that
does not affect the control flow. These attacks are not in our scope or the threat model.
However, this can be addressed by adding instrumentation that also logs the values
of sensitive variables alongside the CFlog [@sun2020oat].

Reliable, Continuous or On-demand delivery of CFreport is orthogonal to this work, but can
be added on top to provide additional guarantees. TA can be configured to send
continuous reports, so that Vrf can detect attacks early on and take action accordingly.
Also, the CFI exception handlers can be configured to report violations, and take
measures to abort execution or perform recovery. This is especially useful
in applications where active attacks are absent, but we need similar guarantees on
execution of the operations. For example, a rover, satellite or a robotic arm controller.

## Compatibility

Although the measures described so far are ARM specific, the concepts themselves
are portable to other platforms and targets.
For example, if PAC is replaced with something else, which provides similar
guarantees, our techniques would still work. Case in point, Intel has Indirect 
Branch Tracking (IBT) for x86-64 which gives instructions that act as branch targets [@intel-ibt].

As for the compatibility of the prototype, if compiled with `-march=armv8a`, it
provides a binary that's backwards compatible with ARMv8-A processors, 
but does not provide the integrity guarantees.
The integrity additions are only fully supported on ARMv8.5-A and later microarchitectures.
So, the prototype is fully compatible with those, while providing only attestation for 
backwards compatible platforms.

This work, as with BLAST, can be applied to other CFA approaches to provide
integrity and/or performance. Similarly, Data Flow Integrity and Data Flow Attestation
techniques are complementary to this.

## Performance

```{r setup-helpers}
geomean <- function(x) exp(mean(log(x)))
geosd <- function(x) exp(sd(log(x)))
```

```{r logevents}
library(knitr)
library(kableExtra)

df <- read.csv("./data/logevents.csv",
               header = TRUE)


col_names = linebreak(c("Embench IoT\nProgram", "Conditional\nBranches", "Unconditional\nBranches", 
              "Loop\nHeaders", "Function\nCalls", "Returns/Exits"))

foot_str = "The number of events is taken with CPU_MHZ=1000, a parameter denoting the number of runs of each top-level benchmark function."

df %>%
  kable(caption = "Number of logging events",
        escape = F,
        format="latex",
        booktabs = TRUE,
        col.names = col_names,
        format.args = list(big.mark = ",")) %>% 
  kable_styling(latex_options = "scale_down",
        bootstrap_options = "striped") %>%
  row_spec(0, align = "c") %>%
  footnote(general_title = "Note.", 
           footnote_as_chunk = TRUE,
           threeparttable = TRUE,
           general = foot_str)
```

```{r logevent-totals}
col_names = linebreak(c("Embench IoT\nProgram", "CFLAT", "OAT",
                      "BLAST", "BLAST\nInter-Procedure"))

total_df = data.frame(program= df$program)

total_df$cflat <- df$conditional_branches + df$unconditional_branches + df$calls +
                  df$ret_exits

total_df$oat <- df$conditional_branches + df$ret_exits

total_df$blast <- df$loop_headers + df$calls + df$ret_exits

total_df$blast_loglevel <- df$calls + df$ret_exits

diff_percent <- ((total_df$blast - total_df$blast_loglevel) / total_df$blast) * 100

diff_25_low = quantile(diff_percent)[2]
diff_median = quantile(diff_percent)[3]

foot_str = sprintf("Inter-Procedure log level has median difference of %.2f%% with %0.2f%% being 25%% lows.", diff_median, diff_25_low)

total_df %>%
  kable(caption = "Total events",
        escape = F,
        format="latex",
        booktabs = TRUE,
        col.names = col_names,
        format.args = list(big.mark = ",")) %>% 
  kable_styling(latex_options = "scale_down",
        bootstrap_options = "striped") %>%
  row_spec(0, align = "c") %>%
  footnote(general_title = "Note.", 
           footnote_as_chunk = TRUE,
           threeparttable = TRUE,
           general = foot_str)
```


```{r cleandata}
library(knitr)
library(kableExtra)

baseline_raw <- read.csv("./data/baseline.csv",
                     header = TRUE)
baseline_pacbti_raw <- read.csv("./data/baseline_pacbti.csv",
                     header = TRUE)
blast_raw <- read.csv("./data/blast.csv",
                     header = TRUE)
pacbti_raw <- read.csv("./data/pacbti.csv",
                     header = TRUE)
blast_optimised_raw <- read.csv("./data/blast_optimised.csv",
                     header = TRUE)

## Helper Functions
cubic_time <- function(x) x$cubic_end - x$cubic_start
crc32_time <- function(x) x$crc32_end - x$crc32_start
edn_time <- function(x) x$edn_end - x$edn_start
huffbench_time <- function(x) x$huffbench_end - x$huffbench_start
minver_time <- function(x) x$minver_end - x$minver_start
st_time <- function(x) x$st_end - x$st_start
ud_time <- function(x) x$ud_end - x$ud_start
matmult_time <- function(x) x$matmult_end - x$matmult_start
mont64_time <- function(x) x$mont64_end - x$mont64_start
nbody_time <- function(x) x$nbody_end - x$nbody_start
nettle_aes_time <- function(x) x$nettle_aes_end - x$nettle_aes_start
nettle_sha256_time <- function(x) x$nettle_sha256_end - x$nettle_sha256_start
primecount_time <- function(x) x$primecount_end - x$primecount_start
sg_time <- function(x) x$sg_end - x$sg_start
tarfind_time <- function(x) x$tarfind_end - x$tarfind_start

get_times <- function(x) {
  data.frame(
   aha_mont64=mont64_time(x),
   crc32=crc32_time(x),
   cubic=cubic_time(x),
   edn=edn_time(x),
   huffbench=huffbench_time(x),
   matmult_int=matmult_time(x),
   minver=minver_time(x),
   nbody=nbody_time(x),
   nettle_aes=nettle_aes_time(x),
   nettle_sha256=nettle_sha256_time(x),
   primecount=primecount_time(x),
   sglib_combined=sg_time(x),
   st=st_time(x),
   tarfind=tarfind_time(x),
   ud=ud_time(x)
   )
}

remove_outliers = function(x) x[!x %in% boxplot.stats(x)$out]
# clean_median = function(x) median(remove_outliers(x))   # not needed right now
# clean_sd = function(x) sd(remove_outliers(x))   # not needed right now

overhead = function(x,y) ((y-x)/x)*100
fmt_overhead = function(x,y) sprintf("%0.3f", overhead(x,y))
fmt_median = function(x) sprintf("%0.3f", x)

p_wilcox_test = function(x,y) wilcox.test(x, y, paired=TRUE)$p.value
filter_wilcox_test = function(x,y) {
  tmp_col <- c()
  for( i in colnames(x) ) {
    p_val = p_wilcox_test(x[, i], y[, i])
    if (p_val > 0.05) {
      tmp_col <- c(tmp_col, sprintf("**"))
    } else {
      tmp_col <-c(tmp_col, fmt_overhead(median(x[, i]), median(y[, i])))
    }
  }
  tmp_col
}

####

baseline_timed <- get_times(baseline_raw)
baseline_pacbti_timed <- get_times(baseline_pacbti_raw)
blast_timed <- get_times(blast_raw)
pacbti_timed <- get_times(pacbti_raw)
blast_optimised_timed <- get_times(blast_optimised_raw)

df <- data.frame(baseline_median=apply(baseline_timed, 2, median))
#df$baseline_sd <- apply(baseline_timed, 2, sd)

df$baseline_pacbti_median <- apply(baseline_pacbti_timed, 2, median)
#df$baseline_pacbti_sd <- apply(baseline_pacbti_timed, 2, sd)

df$blast_median <- apply(blast_timed, 2, median)
#df$blast_sd <- apply(blast_timed, 2, sd)

df$pacbti_median <- apply(pacbti_timed, 2, median)
#df$pacbti_sd <- apply(pacbti_timed, 2, sd)

df$blast_optimised_median <- apply(blast_optimised_timed, 2, median)
#df$blast_optimised_sd <- apply(blast_optimised_timed, 2, sd)

df$baseline_pacbti_overhead <- filter_wilcox_test(baseline_timed, baseline_pacbti_timed)
df$blast_overhead <- filter_wilcox_test(baseline_timed, blast_timed)
df$pacbti_overhead <- filter_wilcox_test(baseline_timed, pacbti_timed)
df$blast_optimised_overhead <- filter_wilcox_test(baseline_timed, blast_optimised_timed)
df$pacbti_vs_blast <- filter_wilcox_test(blast_timed, pacbti_timed)
df$blast_optimised_vs_blast <- filter_wilcox_test(blast_timed, blast_optimised_timed)

# fmt median for cleaner table
df$baseline_median <- fmt_median(df$baseline_median)
df$baseline_pacbti_median <- fmt_median(df$baseline_pacbti_median)
df$blast_median <- fmt_median(df$blast_median)
df$pacbti_median <- fmt_median(df$pacbti_median)
df$blast_optimised_median <- fmt_median(df$blast_optimised_median)


col_names = linebreak(c("Program", " ", "PACBTI", 
              " ", "PACBTI", "PACBTI (Inter)",
              "PACBTI", " ", "PACBTI", "PACBTI (Inter)",
              "PACBTI", "PACBTI (Inter)"))

foot_str = paste("Samples n=100 for each program, along with CPU_MHZ=1000.",
            "(**) values are omitted as the overhead was not significant according",
            "to Wilcoxon Signed Ranked Test.",
            sep = " ")

df %>%
  kable(caption = "Overheads",
        format="latex",
        booktabs = TRUE,
        col.names = col_names,
        align = "lrrrrrrrrrrr",
        format.args = list(big.mark = ",")) %>% 
  kable_styling(latex_options = "scale_down",
        bootstrap_options = "striped") %>%
  row_spec(0, align = "c") %>%
  footnote(general_title = "Note.", 
           footnote_as_chunk = TRUE,
           threeparttable = TRUE,
           general = foot_str) %>%
  add_header_above(header = c(" " = 1, "Baseline" = 2, "Blast" = 3, "Baseline" = 1, 
    "Blast" = 3,"Blast" = 2)) %>%
  add_header_above(header = c(" " = 1, "Median Runtime" = 5, "Overhead (%) over Baseline" = 4,
    "Overhead (%) over BLAST" = 2)) %>%
  landscape()
```

### Benchmark Environment
Due to unavailability of hardware, the limited support for the features, we decided
to use QEMU emulator for evaluation, which has support for the ARM features used
in the prototype [@qemu].
The Environment details are as follows:

- Host:
  - Fedora 41 x86-64
  - Kernel 6.14.4-200
  - Intel Xeon E-2314 @ 2.80GHz 4 Cores, 4 Threads
  - 32 GB dual-channel DDR4 ECC memory running at 3200 MT/s
- Emulator and compilation toolchain:
  - QEMU 9.2.0 monitor, cpu=max
  - GCC v12.2.0 aarch64-unknown-linux-gnu-gcc  
  (crosstool-NG 1.25.0.122_aa6cc4d)
  - Clang v12.0.0
  - OP-TEE Buildroot master branch, `db58889dd1` 31-Jan-2025

Obtained results are expected to be higher compared to physical systems due to
the emulation. Therefore all the benchmarks including baseline were recorded in
this environment, to keep the toolchain a control variable.

We ran Embench IoT Programs for benchmarking, mainly due to the same being used in
evaluating BLAST [@blast] so as to keep uniformity between the works.
The parameter governing the number of runs of each top-level benchmark function,
`CPU_MHZ` was set to `1000`, same as reference. While number of binary runs, or samples
was changed to `100`, increasing from reference's `n=10`.

We ran 5 different versions of the benchmark binaries:

1. Original, called `vBaseline`.
1. Baseline + (PAC + BTI), called `vBaseline-pacbti`.
1. Reference BLAST, called `vBlast`
1. BLAST + (PAC + BTI), called `vBlast-pacbti`
1. BLAST + PAC + BTI Inter-Procedure Loglevel, removing loop log events. Called `vB-optimised`

### Results
The results obtained are summarised in table \@ref(tab:cleandata).
The overhead shown by `vBlast` varies from the original work due to a few reasons:

- Change in toolchain and the benchmark environment. They used a physical Raspberry Pi 3 Model B+.
- Change in the prototype TA. The original one had hashed the logs using BLAKE2s function.
It was changed to SHA256 due to builtin API support from OP-TEE, and unavailability of the reference TA source.
There may be other customizations done by the reference TA unknown to us, which
may have factored in the performance difference.
- Time spent in the TA for the initialization and wrap up have been included, which
may not have been the case for the reference.

Changes between version 2-5 over `vBaseline`, and 4-5 over `vBlast` were taken and were 
tested using Wilcoxon Signed Ranked Test, which is a non-parametric test to
check if there is a significant change between two pairs.
It does not assume normality of data, and is minimally affected by outliers.

Any change that was considered to be insignificant for p-value of `0.05`, has
been marked as (`**`). And for other values, overhead was calculated using
$\frac{y - x}{x} \times 100$. Where `x` and `y` are medians of the respective samples.

Let us first discuss the effect of PAC + BTI over the respective baselines (v1 and v3).
A difference in overheads can be seen when PAC + BTI is added on top of `vBaseline` vs on `vBlast`.
When the runtimes are low, i.e. shorter burst programs, it adds a significant overhead,
but in longer running programs, the overhead is either small or insignificant.
This can be seen in `vBlast-pacbti` vs `vBlast`, where the programs with `>1%` overhead
are the ones with comparatively smaller runtimes.
One thing to note is that although the overhead percentage looks big, the runtimes
are not that different, and the difference is mostly in `< +-1s` range.

Due to the variance in the number of logging events as shown in \@ref(tab:logevent-totals),
the amount of overhead also varies accordingly, for `vBlast` as well as `vB-optimised`.
Our interest lies in the overhead of `vB-optimised` over `vBlast`. We see speedups
in almost all benchmarks, except `cubic` and `nettle_sha256` which saw almost `3-10%`
slowdown. This can be explained by the fact that both saw `>1%` overhead in
`vBlast-pacbti` vs `vBlast`. Program `nettle_aes` also saw the effects, having speedup
in single digit, unlike other programs which had `>10%` speedups.

Overall, we can say that adding integrity guarantees made it possible to remove 
loop entries from logs, and gave us a performance boost over the BLAST reference.
We do not get same speedups across the board, but that is expected considering all
programs are different and have different types of operations affecting the runtimes.
