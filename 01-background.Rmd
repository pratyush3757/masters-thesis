---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Background

\minitoc <!-- this will include a mini table of contents-->

There are digital devices all around us running code written by people. It can be 
to handle a simple electric toothbrush's timer, or to move a robot arm for a surgery.
And, all programs are not created equal. Some are mission-critical pieces of code that 
may require high levels of scrutiny at each step of creation, lest they fail and 
result in disasters - financial or otherwise.

For such a program running on a device, we need to ensure that it ran correctly 
(meaning - as intended by the logic of the code). 
But just writing good code doesn't save it from other factors that are present while running it.
There are a few factors that may influence the program to not behave correctly:[?? CITEME]

1. A bug in the code that slipped past the review process.
2. There's a flaw in the device that the code is running on.
3. A malicious actor attacks the device/program to modify the execution.
4. The device's environment is adversarial. Eg. A nuclear plant's control panel.

[define control flow here...]{highlight="pink"}

For factors 1-2, there are techniques like source code auditing, testing, 
fuzzing, static and dynamic program analysis, and formal verification that try to minimize the software/hardware bugs. 
[@artoftesting; @millerfuzzing; @luk2005dynamic; @godefroid2012sage; @quinlan2009source; @kern1999formal]

For factor 3, there are various types of results that are possible ranging from 
Denial of Service to Information Leak.
Our focus is on preventing the control flow of the program from changing,
and for that, the techniques come under the term Control Flow Integrity (CFI).[@sok]

But what if the adversary is the device's environment like factor 4? 
Or what do we do about the fact that we still cannot guarantee that the program is secure?
As stated by Bruce Schneier: Security is a process, not a product.[@bruceessay]
No system is fully secure, and security flaws are inevitable.
We need to at-least be able to prove in some way that the program behaved as intended.

For simple programs that process an input to give a deterministic output like 
counting prime numbers or calculators, it is easy to just verify the output.
But for non-deterministic or complex programs with side-effects (changes to the 
system or device - like writing to a file, or moving a robot arm), verifying the 
output is not feasible. [One way is to instrument the system to check that the 
control flow of the program is as expected, which in turn may indicate 
that the program ran correctly.]{highlight="pink"} 
And for this instrumentation, the set of techniques is called Control Flow 
Attestation (CFA).

## Control Flow Techniques
Let us now look at both CFI and CFA techniques in detail to know their aims and the current 
capabilities [afforded to us by them.]{highlight="pink"}

### CFI

#### What/Why?
[Edit/Add to this part]{highlight="pink"}

#### History and Techniques used
[Edit/Add to this part]{highlight="pink"}

##### Various (3/4) types of attacks possible and their mitigations

- Code injection
- Code Reuse (ROP/JOP)
- Data Manipulation
- Side channels
- ...

[Mention PAC + BTI discussed in Arm Specifics Section]{highlight="pink"}

### CFA

#### Why?
As noted previously, analysis, testing and even adding CFI doesn't mean the program is secure.
Also, there are many use-cases where we need an unforgeable evidence of the control flow
of a given program. For example, in a syringe pump, where this evidence could be 
needed by hospital and insurance that it indeed delivered the given dosage and correctly at that. 
This can be achieved by path attestation, where the verifier gets a non-repudiable 
proof of execution on prover for the given input.
Path attestation also acts as a lighter alternative to verifiable computation [?? CITEME].

There are other integrity measurements too that can attest to various properties 
of the system being tested.
Like, weather the system satisfies certain properties [?? CITEME], it booted up 
with particular software stack [?? CITEME], or that it executed certain operations correctly [?? CITEME].

The work done on CFA has been parallel to CFI, but with an aligned objective [@sok].
We will focus on path attestation, and whole program path in particular. 
The reason for that is explained later in CFA - Techniques subsection.

#### General Overview of the process
```{r cfa-workflow, fig.cap="Typical CFA Interaction", out.width='50%', fig.align='center', fig.pos='H'}
knitr::include_graphics("./imgs/placeholder-cfa-workflow.png")
```

For CFA, the general workflow has 2 separate agents, a prover (Prv) and a remote verifier (Vrf). 
The Vrf sends a request to Prv with a cryptographic challenge (Eg. a *nonce*).
As a response to the challenge, Prv executes the program with the given 
input (either implicitly or explicitly given in the request).
It then generates a report of execution (CFreport), and sends it to Vrf.
This CFreport is signed using the challenge sent by Vrf, and contains the output of the program and an 
authenticated log (CFlog) of the control flow taken by the program during execution.

*But how would the Vrf know that Prv has not forged the report?* 
This is where a Root of Trust (RoT) comes in.
A Root of Trust (RoT) is a part of the system that we essentially *trust* to work 
properly even in adversarial conditions. This trust in the mechanism is assumed and not derived.
It then serves as the foundation on which the rest of the security guarantees 
of the system can be derived. Commonly, systems have Trusted Execution Environments 
(TEE) like Intel SGX [?? CITEME] or ARM TrustZone [?? CITEME] that act as the RoT.

In our case, it's a piece of hardware on Prv that monitors the system, validates its state, 
and generates the CFlog and CFreport that's sent to Vrf.

When Vrf recieves CFreport, it uses it to determine whether the program was executed as expected by Prv.
It does so by checking if CFlog contains a valid control flow of the program 
for the input. If the path is invalid, Vrf can also analyze CFlog to detect 
deviations and determine the cause.

#### Techniques used{#cfatechniques}
Remote attestation to establish memory content integrity has been explored previously 
in various domains[??Add citations]{highlight="pink"}.
As TEEs were generally historically unavailable on low-end embedded devices, 
software based attestation techniques like SWATT [?? CITEME] and PIONEER [??CITEME] were proposed.
They implemented software based self-checksum functions as a means of attestation.
They relied on precise timing measurements, and were applicable only in settings where
comunnication delay between Vrf and Prv is deterministic. 

Then there are Hybrid attestation techniques, that aim to minimize changes to 
underlying hardware, while giving same guarantees as hardware-based techniques.
VRASED [?? CITEME] adds integrity checks in software, while using trusted hardware to control the checks.

But these techniques measure the state of Prv only when remote attestation is executed.
They are oblivious to program's state between two consecutive measurements, or 
information of program before measurements. That means they suffer from 
time-of-check to time-of-use (TOCTOU) attacks. There are proposed designes that avoid this limitation.[?? CITEME RATA]

Works in path attestation, where the control flow path of the program is recorded 
for the proof, include CFLAT [?? CITEME CFLAT] and OAT [?? CITEME OAT].
We will focus on these two, as they can represent the main approaches to path 
attestation, and other works have essentially similar methods [?? CITEME].

CFLAT computes the hash of basic blocks by adding instrumentation at their end, 
and then accumulates their hash which represents the execution path of the program.
The Vrf then compares it with historically collected expected hashes for known 
execution paths.
OAT on the other hand, adds instrumentation after conditionals, which signifies 
the direction the branches took. This, leads to less instrumentation compared to 
CFLAT where every edge of the CFG is instrumented.
This information is saved in the TEE in the form of a bit-trace, along with 
jump, call, and return addresses.
The Vrf then uses the bit trace and other values to perform a symbolic execution
of the program for verification.
Their instrumentation approaches for the CFG of a program 
are shown in figure \@ref(fig:cflat-oat-instruments).
[Add cflat-oat-instruments.png]{highlight="pink"}

CFLAT and OAT have logging events for each Conditional Branch, Indirect function call, and Function exit.
Additionally, CFLAT also have events for Unconditional Branches and Direct function calls.
All of these events has a domain transition to TEE for logging. Which, when large codebases
are taken in account, can become a performace bottleneck due to the sheer number of them [@blast].

[?? TABLE]{highlight="pink"} describes the number of TEE domain switches at runtime,
as seen for CFLAT and OAT for Embench-IOT benchmarks, when applied to whole programs.
It also shows the amount of extensive instrumentation that is needed for path attestation. 
Despite such intrusive amount of instrumentation, which also translates into 
a proportional number of domain switches, CFLAT and OAT report fairly low runtime overheads.
CFLAT reports runtime overhead in seconds when evaluated on a syringe pump application.
OAT reports an average runtime overhead of 2.7% on 5 embedded applications.

This low overhead for both can be explained by the fact that their application 
has been to small programs or "operations" that span a few thousand control-flow events.
When applied to whole programs, the overhead turns out to be more than 1000x.[@blast]
This means, a program that takes a few seconds would take hours to complete.
Other previous works, like CFLAT and OAT, have mostly been applied to small 
programs or to programs that work in specialized environments [?? CITEME][@blast].
Prior work has also shown that a Vrf can miss attacks that are directed against
a program when paths in only some parts of it are attested [?? CITEME].
Therefore, it's better for Vrf to be able to attest the entire execution of a program.

BLAST [@blast] was then proposed, which overcame the limitations of CFLAT and OAT while
also adding whole program control flow attestation capabilities. It's discussed
in detail later on in [Primary Works - BLAST subsection](#blast).

Also, most CFA techniques do not guarantee that the report will be recieved by Vrf.
The Vrf may not trust the output recieved by it unless CFlog is attached to it,
runtime auditing [?? CITEME, Sok 41,42] aims to bridge that gap and reliably deliver
CFlog even when Prv turns malicious and tries to not follow the protocol.

## ARM Specifics
Now that we know [what CFI and CFA are]{highlight="pink"}, we can take a look at platform specific solutions.
For this work, we choose ARM as it's common in embedded applications[?? CITEME], 
[which is a popular use-case where CFI and CFA techniques are often applied]{highlight="pink"}.[?? CITEME]()

Before diving into the implementation details, we'll first discuss the ARM Architecture
(ARMv8-A specifically) and its calling conventions to brush up the required background knowledge.
We'll also discuss some architecture level features that can be used for CFI and CFA.

### ARM Assembly Basics
We will only focus on AArch64, the 64-bit instruction set, and on the ARMv8-A and later profiles[@arm-arch-manual].

#### Registers
Let us first talk about Registers.
SIMD, floating point and vector registers will be excluded in this section for brevity.

In AArch64, the available registers are:[@arm-isa; @armasm-user-guide]

- 31 64-bit general-purpose registers `x0 - x30` which can hold any type of data.
They usually hold integers and addresses, but anything with length of 64-bits 
can be stored in them. Their lower 32-bits can also be accessed and used as 
32-bit registers by using `wN` instead of `xN`, where `N` is the register number.
- 1 Program Counter `PC`.
- 1 Process State `PSTATE`.
- `wzr/xzr` called zero register,
which reads `0` and ignores the writes. Therefore it's only useful as a source.
It doesn't represent a real register on chip,
but it's just a symbol for "assume value of operand to be zero". 
- 4 stack pointer registers `SP_EL0 - SP_EL3`.
- 3 exception link registers `ELR_EL1 - ELR_EL3`.
- 3 saved program status registers `SPSR_EL1 - SPSR_EL3`. They are 32-bit wide only, 
unlike other registers.

There is no register `w31/x31`. Depending on instructions, the register number `31` 
either represents the zero register or the stack pointer.

Depending on the Exception Level or privileged execution mode, 
the Stack Pointer `SP` is represented by the corresponding `SP_EL` register.

As we are talking about user level code (`EL0`) only, we will ignore the `ELR` and `SPSR` 
registers, and consider `SP_EL` registers to just be `SP`.

The registers and their usage are as follows:[@arm-isa; @arm-pcs; @armasm-user-guide]

- `x0` to `x7`:  
(Volatile) For passing arguments and return values for system and function calls. 
Additional arguments are stored on the stack. 
The return value is typically stored in `x0`. Caller assumes they will be modified.
- `x8`:  
(Volatile) Indirect result location register. It is also typically used for holding 
syscall number.
- `x9` to `x15`:  
(Volatile) General purpose, for storing temporary data. Caller saved.
- `x16` and `x17`:  
(Volatile) Intra-procedure-call registers. Used as scratch 
registers by system linker during calls.
- `x18`:  
Reserved for platform specific use. Usage is avoided for portability.
- `x19` to `x28`:  
(Non-volatile) Callee-saved registers. They must be preserved across calls.
If used, the callee must push the values to the stack at the beginning of function,
and restore them before returning.
- `x29` (Frame Pointer `FP`):  
(Non-volatile) Points to the start of the stack frame of a function.
- `x30` (Link Register `LR`):  
(Non-volatile) Holds the return address of a function call.
Can be used, but value must be restored before returning.
- `SP` (Stack Pointer):  
Special Register. Points to the top of the stack. 
Can be used as base address register for loads and stores.
- `PC` (Program Counter):  
Special Register. Cannot be accessed explicitly.
Holds the address of the next instruction to be executed.
- `PSTATE`:  
Holds the various status and control flags for the processor. 
They include Execution State, Interrupt Mask Bits, Processor Mode, Condition Flags, 

---

*Sidenote:* XNU (Apple iOS and MacOS) ABI's conventions differ from the aforementioned one.
Although private and undocumented, it can be seen from the source [@apple-oss] that
the main differences (as of 02 May 2025) are:[@so-xnu-abi]

- The immidiate value passed to `svc` instruction is ignored. `svc 0x80` is used 
by the std library, which invokes a single handler.
- The register used for syscalls is `x16` instead of `x8`.
- The arguments are in `x0` through `x8` registers, supporting upto 9 arguments. 
This is only used if there's an indirect syscall i.e. `x16 = 0`.
- Stack is not used for passing arguments.
- `x0` and `x1` can hold 2 return values. For example in case of `fork`.
- Error is reported using the carry bit, in which case `x0` would hold 
the error code.
- Syscall numbers can be *negative*. 
UNIX ones are non-negative as usual, ranging between `0` and `557`. 
Mach ones are negative with range between `-10` and `-100`.

---

#### Calling (and Branching) Conventions
Now that we know the details about the registers, their usage and their volatility for a caller,
we can look at how functions are called in ARM architecture.

The main difference here from `x86-64` ISA is that we don't have a `call` instruction.
Instead branching instructions are used for conditionals, loops, and procedure calls.
We mainly have 5 types of Branching instructions:

- `B label` (Branch):  
Unconditional branch to a label within `PC` $\pm$ `128MiB`.
Not a subroutine call or return.
- `B.cond label` (Branch Conditionally):  
Conditional branch to label within `PC` $\pm$ `1MiB`.
Not a subroutine call or return.
- `BL label` (Branch with Link):  
Unconditional branch to label in range $\pm$ `128MiB`, 
and store the return address (`PC+4`) in `x30`.
It is a subroutine call.
- `BR xN`(Branch to Register):  
Unconditional branch to address in register `xN`.
It is not a subroutine return.
- `BLR xN` (Branch with Link to Register):  
Unconditional branch to address in register `xN`, 
and store return address (`PC+4`) in `x30`.
It is a subroutine call.

There are many other branching instructions too, like `CBZ`, `TBZ` etc. 
but they are not relevant for us.

Now, we get 2 types of functions:

- *Non-leaf* - Functions that call other functions.
- *Leaf* - Functions that don't have any function calls in their body, and simply return.

*Why is it relevant?*
Because subroutine calls would use `BL`/`BLR` instructions,
and store the return address in `x30`.[@arm-arch-manual]
We don't need to worry about Leaf functions, 
but what about nested Non-leaf function calls? `x30` would get overwritten.  
To prevent that from happening, we make the distinction, and treat them differently.
For Non-leaf functions, we simply save the value in `x30` at the start of the function, 
and restore it before return. It looks like this:

```asm
foo:
  // save x30
  ...
  bl bar  // call bar
  ...
  // restore x30
  ret

bar:
  // save x30
  ...
  bl baz  // call baz
  ...
  // restore x30
  ret

baz:
  ...
  ret
```

> `foo` and `bar` are non-leaf functions, whereas `baz` is a leaf function.

### Pointer Authentication Code (PAC){#pac-bg}
Pointer Authentication was added in ARMv8.3-A under the mandatory feature `FEAT_PAuth`.[@arm-feat-names]
To understand PAC, we must first understand why and what does it aim to fix.

#### Return Oriented Programming (ROP)

We have [discussed about stack smashing]{highlight="pink"}, and how arbitrary code execution can be prevented.
This means that attackers can't inject and use arbitrary code, but what is stopping 
them from using existing application code? Many applications are huge in size,
and that means there is a lot of code that has execute permissions already.
The attackers can analyze the application, and find out gadgets. Gadgets are executable 
fragments of code, usually ending with a return. For example:
```asm
...
add x0, x1, x2
ret
```
The attackers can then chain these gadgets together to form and execute their malicious code.
Any library that is in the address space of the process is a potential source of gadgets.
For example, GLIBC on ARM64, has almost 16,000 usable gadgets.[@riscyrop]
The ROP exploit process is described in figure \@ref(fig:rop-diag).

```{r rop-diag, fig.cap="Using ROP Gadgets", out.width='100%', fig.align='center'}
knitr::include_graphics("./imgs/placeholder-rop.png")
```

Although the process of finding and chaining the process is hard, it can be automated. 
ASLR can make the process harder, but it is still possible to carry out ROP attacks using
Automated Exploit Generation (AEG) tools or information disclosure.[@aeg2014; @riscyrop]

#### Solution?

ROP exploits the fact that the return address is not checked, and can be used to 
jump to next gadget instead. What if we check the return address for corruption 
before using it? One way is using stack canaries, but even those can be bypassed 
by memory disclosure vulnerabilities.

What if we sign the `LR`? The attacker would need a correctly signed address to replace it, 
increasing the difficulty of exploit generation.
That's exactly what PAC does. Pointer Authentication
takes advantage of the fact that pointers are 64-bit long, but not all 64-bits 
are needed to represent the virtual memory address. 

```{r pac-bits, fig.cap="Pointer Layout on ARM64", out.width='100%', fig.align='center'}
knitr::include_graphics("./imgs/placeholder-pac-bits.png")
```
PAC uses the top bits of the pointer to store a signature of the pointer 
(as shown in fig \@ref(fig:pac-bits)), and they aren't
used as a part of the address. At the start of the function, the `LR` is signed.
Then just before the return, the address is authenticated using PAC. If the check fails,
an exception is raised. Now, to perform a ROP attack, the attacker needs gadget addresses 
(made harder with ASLR), and have properly signed pointers to those addresses.
To get a signed pointer, they would need to access a signing gadget.

After adding the PAC instructions, a function would look like this:
```asm
foo:
  paciasp
  ...
  autiasp
  ret
```
*Note:* If we don't need backwards compatibility, we can use `RETAA` which is equivalent to `AUTIASP + RET`.

#### Using NOP space

A subset of the PAC instructions are in the NOP-space. That means, if a
library or application uses the NOP-space instructions to add PAC support, 
they can still run on old processors that do not support PAC features. 
The instruction would just be treated as a `NOP`.
This is accomplished by encoding the instructions as "hints" to the processor.
These include: `AUTIx1716`, `AUTIxSP`, `AUTIxZ`, `PACIx1716`, `PACIxSP`, `PACIxZ` and `XPACLRI`.
Where (lower) `x` is either `A` or `B` depending on the key to be used.[@sipearl]
More in next section on keys. There are many more instructions, 
but for now we'll just talk about these NOP-space ones.

#### Forming a PAC

ARM provides 5 128-bit keys. Each of them is stored in a pair of 64-bit system registers:

- 2 keys, A and B, for instruction pointers
- 2 keys, A and B, for data pointers
- 1 key for general use

And these registers are only accessible in EL1 and above (refer \@ref(fig:armtz-arch)).
The signature is formed by using the key, address and a modifier as shown in figure \@ref(fig:pac-diag). 
```{r pac-diag, fig.cap="PAC formation", out.width='100%', fig.align='center'}
knitr::include_graphics("./imgs/placeholder-pac-diag.png")
```

The strength of PAC depends on the size of the configured virtual address. 
The keys have limited life spans, that is, each time the application is launched, the keys can be rotated.
And each application can have its own independent set of keys. 
The modifier should be the same for sign-authenticate instruction pair on entry/exit.
So it specifies the context in which the pointer is valid in. For example, 
using `SP` as the modifier would give us a PAC that's valid only for that call of the function, 
as `SP` may be different on future calls.

The attacker would need to get every pointer correct in the chain, otherwise an 
exception would be raised.

[Results discussed in chapter 3]{highlight="pink"}

#### But what do these instructions mean?

- `AUTI` prefix is an Authenticate instruction.
- `PACI` prefix is a PAC computation instruction.
- `A/B` after the prefix determines the key to be used.
- `1716` suffix means address is in `x17`, modifier in `x16`.
- `SP` suffix means address is in `x30`, modifier is `SP`.
- `Z` suffix means address is in `x30`, modifier is `0`.
- `XPACLRI` is used to strip PAC from address in `x30`.

---

*Sidenote:*
From ARMv9.4-A, under feature `FEAT_PAuth_LR`, `PACIxSPPC` and `AUTIxSPPC` 
instructions were added to use `SP + PC` as the modifier to sign `LR`.
It gives better security by tying the caller and callee to the address, 
and uniquely associating PAC with that particular instance of function usage. 

---

At the end, when enabled in the codebase, PAC looks like this:
[Add final example of PAC]{highlight="pink"}
```asm
// source
// generated asm
```

### Branch Target Identification (BTI){#bti-bg}
Branch Target Identification was added in ARMv8.5-A under the mandatory feature `FEAT_BTI`.[@arm-feat-names]
Let us first understand where PAC was still inadequate, and what could be improved by using BTI.

#### Jump Oriented Programming (JOP)
PAC could add protection against ROP attacks by adding checks to ensure a 
function returns to its true caller. That is protected the returning jump points.
But what about calling jump points like indirect branches, function pointers, 
and case statements? JOP attacks use these points to pivot to next gadget in 
sequence instead of `RET`. So JOP gadgets would end with `BR`, `BLR` or similar instructions instead.

`BR` and similar instructions use the register to determine the jump point, which
makes it hard to determine the target statically, or that the target would be valid.
This makes it easy for the attacker to use them to execute their JOP chain.

[fig: JOP.png]{highlight="pink"}

#### Solution?
If it is hard to statically secure the jump points, how about the landing points?
The hijacked jumps could target arbitrary locations to pivot to.
But valid code only jumps to particular locations. So, what if those locations 
can be marked as landing points, and any jump that doesn't land on them becomes invalid?

This is the idea behind BTI. The branch targets are identified and marked with `BTI` 
instructions that act as landing pads. Any jump that does not land on a `BTI` instruction 
generates a Branch Target Exception.[^bti-exceptions]
Function returns are also a type of indirect branch, but they aren't required 
to target the landing pads. Also, for function entry, 
pointer signing instructions `PACIxSP` and `PACIxZ` act like landing pads.

[^bti-exceptions]: `BRK, BTI, HLT, PACIASP` and `PACIBSP` are an exception to this. 
Refer instruction docs for details.[@armasm-user-guide]

The `BTI` instruction has an argument that further specifies what types of indirect branch can it be targeted by:

- `BTI c`: Indirect function calls (`BLR`)
- `BTI j`: Indirect jumps (`BR`), e.g. case-statements
- `BTI jc`: Indirect function calls or jumps

There are some more rules to BTI like processor states for each target instruction type.
Please refer the Sipearl Whitepaper[@sipearl], or Arm Documentation for PAC and BTI[@arm-pacbti] for more details.

#### NOP-space
Instructions added under BTI are also in NOP-space, so that all applications and 
libraries that use it would be backwards compatible with old processors.

[Results discussed in chapter 3]{highlight="pink"}

[Add BTI example here]{highlight="pink"}

### ARM TrustZone
```{r armtz-arch, fig.cap="ARM TrustZone Architecture", out.width='100%', fig.align='center', fig.pos='H'}
knitr::include_graphics("./imgs/placeholder-armtz.png")
```

ARM TrustZone[@armtz] is Arm's security architecture in ARM A-profile architecture.
It provides 2 execution environments with hardware enforced isolation between them:

1. Normal World or Rich Execution Environment (REE) - It runs the regular software 
stack with a complex OS, hypervisor, etc. The attack surface is considered to be large, 
and the execution isn't trusted.
2. Trusted World/Execution Environment (TEE) - It runs a simple and small software 
stack which is assumed to be trusted. It has its own trusted services and OS.
It also has a smaller attack surface due to its simplicity and size. 

There are 2 security states ARM architecture - Non-secure and Secure which map 
to Rich and Trusted worlds respectively. 
Exception levels are the levels of software execution privileges. 
These mappings are specified in figure \@ref(fig:armtz-arch). 
The states are often written with their exception levels like:

- `NS.EL2`: Non-secure state, Exception level 2
- `S.EL2`: Secure state, Exception level 2

*Note that EL3 is always in Secure State.*

```{r smc-convention, fig.cap="smc instruction call convention", out.width='75%', fig.align='center', fig.pos='H'}
knitr::include_graphics("./imgs/placeholder-smc-workflow.png")
```

This isolation is bridged by the usage of Secure Monitor Call (SMC) instruction.
The `smc` instruction not available in EL0 in either of the Security States.
The processes in Rich OS would call the user-space library (TEE Client),
which would in turn perform a low level call to the TrustZone driver in Rich OS.
The driver would then perform a `smc` call with arguments to the Secure Monitor.
The Secure Monitor determines the matching handler in TEE and dispatches it accordingly.
This process is shown in figure \@ref(fig:smc-convention).

### OP-TEE

One example of a Trusted Kernel is OP-TEE. It was developed by ST-Ericsson, 
and is now an open source project maintained by Linaro.[@optee].
The rich applications call OP-TEE Driver through TEE Client API. The Driver 
is then responsible for the communication with the OP-TEE kernel.
```{r optee-arch, fig.cap="OP-TEE Architecture", out.width='75%', fig.align='center', fig.pos='H'}
knitr::include_graphics("./imgs/placeholder-optee.png")
```

## Primary Works

### BLAST
As mentioned in [CFA - Techniques](#cfatechniques), attestation has rarely targeted whole programs.
And their approaches did not scale when applied to whole program paths, mainly 
due to the sheer number of domain transitions between REE/TEE.[@blast]
BLAST[@blast] was developed in 2023 with whole program paths in mind.
They took inspiration from program profiling literature to decrease the said overheads.

#### How it works
[It is a LLVM pass...]{highlight="pink"}

BLAST reserves 2 registers called `BLReg` and `LogReg` for the required instrumentation.

##### Local Logging:
If we need fine-grained logging, but cannot afford the domain switches, why not log inside the REE?
BLAST does this by pre-allocating a log inside the process's address space,
and adds SFI measures discussed in [SFI](#blastsfi) to protect its integrity.
The log is shared between REE and TEE, and its permissions are managed by the TEE.
It is then divided in 2 halves, with each half ending with a write-protected page called `sentinel page`.
`LogReg` is a pointer to the head of the log, and BLAST simply adds the log entry 
and increments its value.

```{r blast-log-pages, fig.cap="Structure of the log in BLAST", out.width='60%', fig.align='center', fig.pos='H'}
knitr::include_graphics("./imgs/placeholder-blast-pages.png")
```

When a half is filled, `LogReg` reaches a sentinel page and a hardware fault is generated on write.
The fault handler is then executed in the TEE, which:

1. Makes the `LogReg` point to the other half.
2. Adds write-protection to the filled half, to ensure it isn't overwritten.
3. Commits the log's state to the TEE.
4. After the log is committed, it makes the half writable again.

##### Software Fault Isolation (SFI):{#blastsfi}
Sentinel pages are write-protected, but what about the regular log? 
Although `LogReg` is reserved, and cannot be used by other parts of the code,
simple store instructions can still modify the log value. Only the instrumentation 
instructions inserted as part of the BLAST pass should be allowed to append to the log.

To ensure this, BLAST adds instrumentation after each store instruction to:

1. Fetch the store address and the log's address.
2. Check that the store address doesn't lie inside the log.

```asm
str w8, [x29, #4]                 // perform the store
add x9, x29, #4                   // obtain the store address
and x9, x9, #0x7ffffffffff00000   // mask the store address
ldr x10, log_start_addr           // 1MB-aligned log start
subs x9, x9, x10                  // if equal, then abort.
b.e _abort
```

The log size is statically defined, and it is also aligned to that size. 
i.e. a 1MB log would be aligned to 1MB address. This makes the check equivalent 
to a mask and subtract.

The reason for adding the check *after* the store is to further ensure that a 
CFI attack would not be able to bypass the check by jumping to the store.
The check would necessarily be run after the store, and therefore aborting the 
execution if the log had been modified.

##### Optimizing log entries:
BLAST takes inspiration from profiling literature to place instrumentation in 
an optimal fashion. It uses Ball-Larus algorithm [@balllarus] to selectively 
add instrumentation to the edges of a CFG. The instrumentation manipulates the value of a counter, 
which at the end of the CFG would be between `0` and `N-1`. Where `N` is the number 
of acyclic paths in the CFG. This counter can be used to uniquely identify the 
runtime path that was taken by the function.

This results in `2x` compact representation of paths compared to OAT's bit tracing, 
and it places instrumentation at fewer locations.[@blast]

##### WPP:
BLAST then takes the large log, and compactly represents it using a context-free 
grammar called WPP. It is an extension of Ball-Larus numbering, to build a single 
control flow path through the complete program.[@blast; @laruswpp]
This takes the log, that can grow upto MBs or even GBs in size, and converts into 
a WPP representation that's just a few hundred bytes big.

#### Outcome
[Edit/Add to this part]{highlight="pink"}

#### Limitations
The thread responsible of committing the log to TEE would have to run in parallel 
to the program. Single core settings had an average runtime overhead of 175% 
as compared to 67% of multi-core execution.[@blast]

Reserving 2 registers may cause compatibility issues, especially with programs 
that rely on inline assembly. In which case, BLAST would not compile.

Also, the binary size overhead can be from 64% upto 167% with function inlining.
This would not be reasonable in certain applications or scenarios like lightweight embedded devices.

### CFA+

#### How it works
[Edit/Add to this part]{highlight="pink"}

- Custom PAC
- BTI
- Logging?

#### Outcome
[Edit/Add to this part]{highlight="pink"}

#### Limitations
[
One of the main limitations of CFA+ is that they do not mention what's their 
(logging policy)?? for attestation. They do not elaborate on whether they have 
fine-grained or course-grained CFA, or even what is their logging mechanism.
The benchmarks report performance that is too good for the number of domain 
switches they'd need for committing the logs in TEE.
]{highlight="pink"}
Also, the attestation architecture diagram given indicates that the attestation 
agent is triggered on BTI violation, but the [text..where??]{highlight="pink"} 
mentions logs to have a violation field. The field indicates weather an entry was due to violation.
This contradiction is not addressed in their paper.[verify this statement]{highlight="pink"}

[rolling their own CFI which has XOR loophole (verify!)]{highlight="pink"}
